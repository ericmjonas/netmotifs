%\documentclass{nature}
\documentclass{article}
\usepackage[english,american]{babel}
\usepackage[backend=biber,style=nature]{biblatex}
\usepackage{todonotes}
\usepackage{subfiles}
\usepackage{grffile}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{multicol}
\usepackage{csquotes}
\usepackage{pdfpages}
\usepackage{placeins}

\renewcommand{\vec}[1]{\mathbf{#1}}


\addbibresource{connectomics motif.bib}
\addbibresource{connectomics motif-relational models.bib}

\DeclareGraphicsRule{.ai}{pdf}{.ai}{}


\title{Discovering cell type motifs in connectomics data}
\author{Eric Jonas, Konrad Kording}

\begin{document}
\maketitle


% \begin{affiliations}
%   \item Something Something
%   \item Something Something
% \end{affiliations}

\begin{abstract}
  New techniques produce massive data about neural connectivity,
  necessitating new analysis methods to discover underlying
  structure. It has long been assumed that discovering the local patterns of microcircuitry is the key to understanding neural function. Here we developed a
  nonparametric Bayesian technique that identifies neuron types and circuitry
  patterns in connectomics data. We show that the approach recovers
  known neuron types in the retina, reveals interesting structure in
  the nervous system of c. elegans, and automatically discovers the
  structure of microprocessors. Our approach extracts structural meaning from connectomics, enabling new approaches of deriving anatomical insights from these emerging datasets.


\end{abstract}

\section*{Introduction}
Emerging techniques \autocite{Morgan2013,Zador2012} promise to
quantify the location of each neuron within a volume of interest,
along with its connections to all other neurons. Far exceeding the
capacity of neuroanatomists to trace small circuits, this leads to
enormous datasets that quantify aspects of the nervous system. The impact of
this kind of high-throughput transition has been seen before -- 
rise of sequencing techniques necessitated the
development of novel computational methods to understand genomic
structure, ushering in an era of bioinformatics as an independent
discipline \autocite{Koboldt2013}.

The brain consists of multiple kinds of neurons, each of which is
hypothesized to have a specific role in the overall
computation. Neuron types differ in many ways, e.g. chemical or
morphological, but they also differ in the way they connect to one
another. In fact, the idea of well defined local connectivity patterns
has been prominent in many areas, ranging from sensory (e.g. retina,
\autocite{Masland2001} via processing (e.g. neocortex \autocite{Mountcastle1997})
to movement (e.g. spinal cord) \autocite{Grillner2005}. Indeed, such
repeated patterns also exists in human-made computing circuits. It
remains an important challenge to develop algorithms to use anatomical
data, e.g. connectomics, to back out underlying connectivity motifs.

The discovery of structure is a crucial aspect of network
science. Early approaches focused on global graph properties, such as
the types of scaling present in the network \autocite
{WattsStrogatz1998} .  While this approach leads to an understanding
of global network, more recent work aims at identifying very small-scale
repeat patterns, or “motifs” in networks\autocite{Milo2002}

The discovery of structure in probabilistic graphs is a crucial aspect
in machine learning. Commonly used algorithms include
community-based-detection methods \autocite{Girvan2002}, and
stochastic block models \autocite{Nowicki2001}.  While these
approaches can incorporate the probabilistic nature of neural
connections \autocite{Hill2012} they do not generally consider the
known fact that the probability of connections between neurons depends
on distance. Combining such probabilistic models with a spatial
component to model distance promises to improve the detection of
microcircuitry from neural data.

When it comes to neuroscience and other computing systems, we expect
patterns of connectivity much more complex than traditional motifs,
exhibiting a strong spatial dependence arising from the complex
genetic, chemical, and activity-based neural development process. 


Here we describe a Bayesian non-parametric model that can discover
circuit structure automatically from connectomics data: both the cell
types and their spatial patterns of interconnection. We apply it to
three computing systems: the mouse retina connectome \autocite{Helmstaedter2013}, the
c. elegans connectome \autocite{White1986}, and a ``connectome'' of a classical
microprocessor \autocite{James2010}. Note we do not explicitly
consider cellular morphology, absolute position (cell layer), or any
chemical or genetic information. Yet in all cases, we discover
cell-type to cell-type connectivity that depends on spatial distance
and these connectivity functions depend on the two involved cell
types. Comparing the cell types discovered by the algorithms with
those obtained by anatomists based on orthogonal information reveals a
high degree of agreement. We present a scalable approach to infer
microcircuitry from connectivity data.

\begin{figure}
  \centering 
    \centerline{\includegraphics[width=183mm]{overview.ai}}
  \caption{An overview of our method. a. Input data is
an adjacency matrix indicating whether cell $i$ (row)
synapses on cell $j$ (column). The location of each cell in space is known. 
b. Our algorithm discovers hidden cell types in this connectivity data
by assuming all cells of a type share a distance-dependent connectivity 
profile, c.), with cells of other types. This results in a clustering 
of the cells by those hidden types. b.) shows the cell adjacency
matrix with cells of the same type grouped together. d.) shows
the learned probability of connection between our different types
at various distances, illustrating short-range and long-range
connectivity patterns.}
\label{fig:overview}
\end{figure}

\section{Results}
We use a Bayesian structure discovery approach that is based on
assumptions about the way positions and cell types affect the
probability of connections between cells, a so-called generative
model. The probability of connections between pairs of neurons
decreases with distance, but how high the baseline probability of
connection is and how quickly it decays with distance depends on the
two cell-types. We combine annealing with Markov Chain Monte Carlo to
perform simultaneously joint posterior inference over the structure,
parameters, and hyperparameters of the model (see Methods). This
procedure automatically learns the number of cell types, which cells 
belong to which types, along with the 
the distance and type dependent connectivity probabilities (see
fig~\ref{fig:overview}). These results can then be compared with the
results of previous anatomical studies that have used other aspects of
neurons, such as chemical properties and cell-morphology to divide
neurons into types.

Before we can use a probabilistic model to analyze real data we first
need to assess if the model works properly. We thus started out by
simulating data for which we know the correct structure and comparing
the estimated structure based on the algorithm (see methods) with the
one we used for simulation. We find that the model does a good job of
recovering the structure we used to generate the data
(fig~\ref{fig:synthetic}a) Indeed, it does far better than a simpler
and well established model (the infinite stochastic block model, or
ISBM) that assumes that there is no distance dependence to the
connectivity. The model converges relatively quickly to an estimate of
the most probable values for the cell types, which is enabled by using
a combination of simulated annealing and parallelized Markov chain
Monte Carlo (see methods for details). It appears that the model
behaves the way it should and allows application to relatively large
datasets.

\begin{figure}
  \centering 
  \centerline{\includegraphics{synthetic.ai}}
  \caption{Correct recovery of true numbers of hidden classes
in synthetic data. a.) Other models over-cluster the number
of classes as they fail to take distance into account, whereas
our model finds close to the true number of classes. b.) ARI, a measure
of ``correct clustering'', for
different true class counts, between our model and a traditional
ISBM. c.) For synthetic data, 
the IRM over clusters, finding a large number of spatially-localized types. 
In contrast, the spatial-relational model recovers the true spatial
extent of the underlying types.}
\label{fig:synthetic}
\end{figure}

\subsubsection{Mouse Retina}
The retina of the mouse \autocite{Masland2001} is an example where we
should expect to have connectivity patterns that are well approximated
by our generative model. It is known that there are multiple classes
of cells that can be grouped into ganglion cells that transmit
information to the rest of the brain, bipolar cells that connect
between different cells, and amacrine cells that feed into the
ganglion cells (cite some review). Recent research \autocite{Helmstaedter2013} has
produced a large dataset containing both the types of cells from
orthogonal approaches, and also the connectivity matrix between all
reconstructed cells. We want to know to which level we can classify
neurons into types based on this connectivity only.

The algorithm took 8 hours to converge to a locally optimal solution,
dividing neurons into a set of cell types (fig~\ref{fig:mouseretina}c, each
wedge is a type). For each pair of neurons there is a specific
distance dependent connection probability (fig~\ref{fig:mouseretina}b,c,d),
which is well approximated by the model fit. Moreover, each type of
cell is rather isotropically distributed across space
(fig~\ref{fig:mouseretina}e) as should be expected for true cell types. The
algorithm is thus able to extract meaningful structure from the data.

Comparing the results of the algorithm to other information sources
allows evaluating the quality of the type determination. Our types
closely reflect the (anatomist-determined) segmentation of cells into
retinal ganglion, narrow amacrine, medium/wide amacrine, and bipolar
cells (fig~\ref{fig:mouseretina}c, outermost ring). We find that the
types we find tend to reflect the known laminar distribution in the
retina (fig~\ref{fig:mouseretina}c, middle ring), which is exciting as
the algorithm did not actually use this type of information.


The algorithm yields a separation of neurons into a smaller number of
types than the fully granular listing of 71 types found by the
original authors of the paper, although is still highly correlated
with those finer type distinctions (see
supp~\ref{supp:mouseretina}. It is our expectation that, with larger
datasets, even closer agreement would be found.

\todo{Do we assign any of the unknown cells correctly?  are types 72-79
“different” or simply unknown?}

We compared the results of our model to the traditional ISBM to see if
the addition of spatial information was necessary. We find that, as
expected from our synthetic experiments, the IRM massively
over-segmented the data, finding spatially-localized “communities” of
cells which bore little relation to known types.  The average spatial
extent of the types found by the ISBM was $14.6\pm8.1 \mu m$. However,
in the ground truth by anatomists the average distance was $34.3\pm7.6
\mu m$ and $35.3\pm 2.4 \mu m$, quite close to the $34.5\pm 1.0 \mu m$
discovered by our algorithm. Modeling the distance-dependence prevents
over-segmentation and thus allows more meaningful estimates.

% from experiments/mouseretina/spatial_var.txt

% Relational Model: mean = 14.586508 std=8.136334
% Spatial-Relational Model: mean = 34.540355 std=1.069483
% Truth (fine): mean = 34.348443 std=7.567564
% Truth (coarse): mean = 35.276636 std=2.372951

\begin{figure}
  \centering 
  \centerline{\includegraphics{mouseretina.ai}}
  \caption{Discovering cell classes in the mouse retina connectome. 
a.) Input adjacency data for 950 cells for which soma positions were known. b. clustered adjacency matrix. c.) connectivity diagram showing our clusters, as
well as the cell depth and known cell type of each cell. Our model agrees
with the cell types traditionally identified by anatomists, and identifies 
laminar-specific connectivity patterns. d.) Connectivity between our
clusters as a function of distance -- the cluster consisting primarily of
retinal ganglion cells (lower left-center) exhibits the expected near and
far connectivity.}
\label{fig:mouseretina}
\end{figure}

\subsection{C. elegans}

The roundworm Caenorhabditis elegans is a model system in
developmental neuroscience\autocite{White1986}, with the location and
connectivity of each of 302 neurons deterministically known. This
incredible cross-organism consistency led to early mapping of the
c. elegans chemical and electrical (gap junction) connectome. Unlike
the retina, only the motor neurons in c. elegans exhibit regular
distribution in space, being distributed along the body axis. Most
interneurons are concentrated in various ganglia that project
throughout the entire animal, and the sensory neurons are primarily
located in a small number of anterior ganglia. 

Using both the chemical and electrical connectivity (see methods), we
determined the underlying clusters explained by connectivity and
distance (fig~ref{fig:celegans}a). A superficial inspection of the results shows
clustering into groups consisting roughly homogeneously of motor
neurons, sensory neurons, and interneurons. Closer examination reveals
agreement with the classifications originally outlined by Brenner in
1986.  

We identify cell types that reflect the known motor/non-motor neuron
classes, even though this system lacks the strong repeat
microcircuitry our model was designed for.  Motor-neuron types AS, DA,
and VA, all exclusively postsynaptic, are clustered together, as are
motor-neuron types VD and DD. Traditional types VC, DB, and VB also
mostly share a cluster. Various head motor neurons, including types
SMD, RMD, and RIM are clustered together. Interneurons with known
anatomically-distinct connectivity patterns, such as RIA (2 cells) or
AVA (2 cells) are clustered into pure types. The algorithm even
correctly places the single-cell type DVB by itself. Note our
clustering does not perfectly reflect known divisions -- several
combinations of head and sensory neurons are combined, and a
difficult-to-explain group of 2 VD, 2 VB, a VA, and a DD neuron are
grouped together. Our identified cell types thus reflect a
``coarsening'' of known types, based entirely on connectivity and
distance information, even when the organism exhibits substantially
less spatial regularity than the retina. 


\begin{figure}
  \centering 
  \centerline{\includegraphics{celegans.ai}}
  \caption{Discovering cell types in the \textit{c. elegans}
    connectome. a.) Initial cell connectivity matrix, red are
    (directional) chemical synapses, blue are electrical
    synapses. Point size is proportional to synapse count between
    those cells. b.) Clustered adjacency matrix showing identified
    clusters. c.) Inter-cluster connectivity, showing the position of
    each cell along the body axis as well as the ``true'' cell type --
    both coarse (motor, sensory, and interneuron) as well as fine
    (small text labels). }
  \label{fig:celegans}
\end{figure}

\subsection{Microprocessor}

To show the applicability of our method to other connectome-style
datasets, we obtained the spatial location and interconnectivity of
the transistors in a classic microprocessor, the MOS Technology 6502
(used in the Apple II) \autocite{James2010}. Computer architects use
common patterns of transistors when designing circuits, with each
transistor having a ``type'' in the circuit. We identified a region of
the processor with complex but known structure containing the primary
8-bit registers X, Y, and S (figure).

Our algorithm identifies areas of spatial homogeneity that mirror the
known structure in the underlying architectural circuit, segmenting
transistor types recognizable to computer architects. Using the
original schematics, we see that one identified type contains the
``clocked'' transistors, which retain digital state. Two other types
contain transistors with pins C1 or C2 connected to ground, mostly
serving as inverters.  An additional identified type controls the
behavior of the three registers of interest (X, Y, and S) with respect
to the SB data bus, either allowing them to latch or drive data from
the bus. The repeat patterns of spatial connectivity are visible in
figure~\ref{fig:microprocessor}a, showing the man-made horizontal and
vertical layout of the same types of transistors.



\begin{figure}
  \centering 
  \centerline{\includegraphics{mos6502.ai}}
  \caption{Discovering transistor types in the MOS 6502
    microprocessor. a.) The clusters and connectivity versus distance
    for connections between Gate and C1, Gate and C2, and C1 and C2
    terminals on a transistor. Red and teal types have a terminal
    pulled down to ground and mostly function as inverters. Purple
    class are clocked, stateful transistors, orange control the ALU
    and yellow control the special data bus. b.) The spatial
    distribution of the transistors in each cluster. }
\label{fig:mos6502}
\end{figure}


\section{Discussion}
We have presented a machine learning technique that allows cell types
and microcircuitry to be discovered using only distance and
connectivity data. We have shown its applicability to a newly
published connectomics dataset from the mouse retina, the classical
connectivity of the worm c. elegans, and the connectivity of a
historical man-made microprocessor. We have found that distance and
connectivity alone are sufficient to discover cell types and
microcircuitry that were known to exist in the systems based on decades
of previous research which had utilized extensive additional
information.

For probabilistic models like ours, no known solution exists to
exactly find the most probable parsing of the neurons into cell-types
and connectivity patterns . We employ a collection of Markov-chain
Monte carlo techniques [reference methods] but while different
initializations converge to similar ultimate values, we can never
realistically obtain the global optimum. There are a broad range of
techniques that may offer better approximations to the global optimum
\autocite{Wanga2012,FritzJonathan} and future work could adapt them to
the problem we addressed here.

Our inference becomes slower as the amount of data increases. Our
algorithm required 30 hours for 1000 neurons and could take months for
substantially longer datasets. Scaling this class of probabilistic
model is an active area of research, and recent results in both
variational methods \autocite{Hoffman2013} and spectral learning
  \autocite{Anandkumar2012} promise substantial speed increases.


  Larger datasets will generally allow algorithms to distinguish more
  distinct types (cite supplemental data) and more data should
  algorithms to get closer to the results from anatomy. Moreover, in
  general, for such problems precision increases with the size of the
  dataset and the cells that we have are not sufficient to
  statistically distinguish all the cell types known in anatomy
  ($\sim 70$). Still, using only connectivity and distance it is possible
  to meaningfully divide neurons into types.

Our small collection of hand-selected distance-dependent likelihood
functions are clearly non-exhaustive, and assume monotonicity
of connectivity probability -- for a given class, closer cells
are never less-likely to connect. This is known to be insufficient
for various neural systems. Future models could incorporate
a wider variety of likelihood functions, or even learn the global
functional form from the data. 

There have existed a range of previous approaches to the discovery
of neural microcircuitry \autocite{missing} These generally involve a great deal
of manual labor and ad-hoc determination of what constitutes a “type”
of cell -- to this day there are disagreements in the literature as to
the “true” types in the mammalian retina. Much as phylogenomics has
changed our understanding of animal ontologies, modern large scale
data will allow the efficient unbiased discovery of cell types and
circuits. The sheer amount of available data demands the introduction
of algorithmic approaches.

The development of automatic identification and quantification of cell
type may also provide a new computational phenotype for quantifying the effect
of disease, genetic interventions, and developmentally-experienced neural activity. Our method can in principle identify neuron-types across non-connected graphs, e.g. across animals. For example, the types of neurons in one animal can be associated with the types of neurons in another animal, in the same way as this is already possible through molecular markers \autocite{Brown2009}. This could be particularly important if cell types appear that are due to properties of the stimuli and experience as opposed to the molecular properties of cells \autocite{missing}. This would allow comparative quantitative anatomy across animals, and the search for the causes of the ultimate connectivity.

Ultimately, we will need to combine information across different
sources into a joint understanding of neural function. Distinct cell
types differ in morphology, connectivity, transcriptomics, relation to
behavior or stimuli and many other ways. Algorithms based on each type
information can then be combined to synthesize all the available
information from one experiment or even across experiments into a
joint model of brain function.

\printbibliography


\newpage
\section{Methods }
\subsection{Methods Summary}

We take as input a connectivity matrix $R$ defining the connections
between cell $e_i$ and $e_j$, as well as a distance function $d(e_i, e_j)$
representing a (physical) distance between adjacent cells. See the
supplemental material for extension to multiple connectivity
matrices. We assume there exist an unknown number $k$ of latent
(unobserved) cell types, $k \in \{1, 2, 3, \dots\}$, and that
each cell $e_i$ belongs to a single cell type, $c_i = k$. The observed
connectivity between two cells $R(e_i, e_j)$ then depends only on
their latent type and their distance through a link function $f(\cdot, d(e_i, e_j))$. We assume $f$ is parameterized based on the latent class, $c_i=m$ and $c_j=n$, via a parameter$\eta_{mn}$, as well as a set of global hyper parameters $\theta$, such that the link function is $f(d(e_i, e_j) | \eta_{mn}, \theta)$. 

We then jointly infer the maximum a posteriori (MAP) estimate of the
class assignment vector ${c_i}$, the parameter matrix $\eta_{mn}$, and
the global model hyperparameters $\theta$ :

\begin{equation}
  p(\vec{c}, \eta, \theta | R ) \propto \prod_{i, j} p(R(e_i, e_j) | f(d(e_i, e_j) | \eta_{c_ic_j}), \theta) \prod_{m, n} p(\eta_{mn} | \theta)  p(\theta) p(\vec{c} | \alpha) p(\alpha) p(\theta)
\end{equation}


% Methods section 3000 words
% methods summary does not appear in the online text
% methods section can have no figures or tables [grrr]


\subsection{Probabilistic Model}

Our model is a extension of the ISBM
\autocite{Kemp2006a,Xu2006} to incorporate spatial relations between entities,
inspired by attempts to extend these models with arbitrary
discriminative functions\autocite{Murphy2012}.


We describe the spatial ``Logistic-distance Bernoulli''  function here,
and others in the supplemental material. 

% fixme class component, NOT class
The ``logistic-distance Bernoulli'' spatial model assumes that, if cell
$e_i$ is of class $m$ and cell $e_j$ is of class $n$, then $\eta_{mn}
= (\mu_{mn}, \lambda_{mn})$, and the probability that two cells $e_i$
and $e_j$ are connected is given by
\begin{eqnarray}
p^* &=& \frac{1.0}{1 + \exp \frac{d(e_i, e_j) - \mu_{mn}}{\lambda_{mn}}}\\
p &= & p^* \cdot (p_{max} - p_{min}) + p_{min}
\end{eqnarray}
where $p_{max}$ and $p_{min}$ are per-graph hyperparameters. Per-component parameters $\mu_{mn} \sim \exp(\mu | \mu^{hp})$ and $\lambda_{mn} \sim \exp(\lambda | \lambda^{hp})$. 

We use a Dirichlet-process prior on cluster assignments, which allows
the number of clusters to be determined automatically. In brief, the
probability of a cell belonging to a cluster is proportional to the
number of datapoints already in that cluster, $m_k$, such that $p(c_i
= k) \propto \frac{m_k}{N + \alpha}$ and the probability of the cell
belonging to a new cluster $k'$ is $p(c_i = k') \propto
\frac{\alpha}{N + \alpha}$.


\subsection{Inference} 
We perform posterior inference via Markov-chain Monte Carlo (MCMC),
annealing on the global likelihood during the traditional burn-in
phase. MCMC transition kernels for different parts of the state space
can be chained together to construct a kernel whose ergodic
distribution is the target ergodic distribution over the entire state space. 

Our first transition kernel (``structural'') performs gibbs sampling 
of the assignment vector $p(\vec{c} | \eta, \theta, \alpha)$. 
The lack of conjugacy in our likelihood model makes an explicit 
evaluation of the conditional assignment probabilities impossible, 
motivating us to use an auxiliary variable method \autocite{Neal2000}
in which a collection of ephemeral clusters are explicitly represented
for the duration of the Gibbs scan. 

We then employ a transition kernel to update the per-component
parameter values $\eta_{mn}$. Conditioned on the assignment vector
$\vec{c}$ and the model hyperparameters $\theta, \alpha$ the 
individual $\eta_{mn}$ are independent. We slice sample \autocite{Neal2003}
each component's parameters, choosing the slice width as a function
of the global hyperparameter range. 

The global hyper-parameters, both $\alpha$ and $\theta$, are allowed
to take on a discrete set of possible values. As $\theta$ is often a
tuple of possible values, we explore the cartesian product of all
possible values. We then Gibbs sample, which is always possible in a 
small, finite discrete state space. 

We chain these three kernels together, and then globally
anneal on the likelihood from a temperature of $T=64$ down to 
$T=1$ over 300 iterations unless otherwise indicated, and
then run the chain for another $100$ iterations. 

To pick the MAP, we run up to $100$ simultaneous chains in parallel,
initialized from different random initial points in the state space,
and then pick the chain with the highest log likelihood. Early
experiments attempted to use the full ensemble of chains as an
estimate of the posterior but were abandoned due to the difficulty of
summarizing the posterior over such a complex state space, which is an
ongoing area of research in the probabilistic modeling community
\autocite{missing}.


\subsection{Validation}


\subsection {Mouse Retina}
Dense serial electron microscopy of a $VOLUMNE$ in the mouse retina by
\autocite{Helmstaedter2013} yielded a listing of places where neurons
come into contact. There were over 1000 cells originally, and selected
the $950$ for which the location of the soma could be reconstructed
from the provided cell plots (soma locations were not provided by the
study's authors in machine-readable form). Ultimately
this left a matrix between the total synapse-like contact area between
all pairs of 950 cells. Area was thresholded at $0.5\mu m$, determined
by hand, to yield a 950 $\times$ 950 entry matrix that served as input
to our algorithm. We measured the distance between cells using
the reconstructed soma centers, and used the Logistic-Distance
spatial relation. Hyperprior griddings are shown in \ref{supplemental}. 

\subsection{C. elegans}

We obtained the connectome of c. elegans from \autocite, and isolated
the 279 nonpharyngeal neurons, with a total of 6393 chemical synapses
and 890 gap junctions originally cleaned up in \autocite{Chen2006}. A
cell's position was its distance along the anterior-posterior axis
normalized between zero and one. We used both networks, the chemical
network as a directed graph and the electrical network as undirected
graph. We use the synapse counts with the logistic-distance poisson
likelihood, scaling the counts by 4.0 to compensate for 
the Poisson's overdispersion. 

\subsection{Microprocessor}
We extracted the connection graph for the transistors in the MOS6502
\autocite{visual6502source}. Each transistor has three terminals (gate,
source, drain), but the methods of the original dataset were unable to
consistently resolve which of the C1 and C2 terminals were source and
drain, leading to ambiguity in our encoding. We identified a region
consisting of consisting of three registers X, Y, and S via visual
inspection and focused our efforts there. We created a total of six
connectivity matrices by examining possible terminal pairings. One
graph, for example, $R^{gc_1}(e_i, e_j)=1$ if transistor $e_j$ and
$e_j$ are connected via pins $g$ and $c_1$. 


% \begin{addendum}
%  \item Put acknowledgements here.
%  \item[Competing Interests] The authors declare that they have no
% competing financial interests.
%  \item[Correspondence] Correspondence and requests for materials
% should be addressed to A.B.C.~(email: myaddress@nowhere.edu).
% \end{addendum}
\newpage
\section{Supplemental}

\subsection{Other Likelihoods}
\label{supp:otherlikelihoods}

We reparameterized the Logistic-Distance Bernoulli likelihood to
better capture the microprocessor data structure. We explicitly
setting the maximum probability $p$ of the logistic function on a
per-component basis, drawing from a global $p \sim Beta(\alpha_{hp},
\beta_{hp})$. Then $\lambda$ is set for each component as a global
hyperparameter, $\lambda$.


The ``logistic-distance Poisson'' spatial model assumes that, if cell
$e_i$ is of class $m$ and cell $e_j$ is of class $n$, then $\eta_{mn}
= (\mu_{mn}, r_{mn})$, and the probability of $R(e_i, e_j) = c$ synapses between two cells $e_i$ and $e_j$ is given by:
\begin{eqnarray}
\lambda^* &=& \frac{1.0}{1 + \exp \frac{d(e_i, e_j) - \mu_{mn}}{\lambda}}\\
\lambda & = & \lambda^* \cdot (r_{mn} - r_{min}) + r_{min} \\
R(e_i, e_j) \sim Poi(c | \lambda)
\end{eqnarray}

where $\lambda$ and $r_{min}$ are per-graph parameters. Per-component parameters $\mu_{mn} \sim \exp(\mu | \mu^{hp})$ and $r_{mn} \sim \exp(r_{mn} | r_{scale}^{hp})$. 

\subsection{Source code}

All source code and materials for running experiments can be
obtained from the project website, at http://spatialrelation.github.io. 

\subsection{Extension to multiple graphs}
\label{supp:multigraph}
The model can handle multiple graphs $R^q$ simultaneously with a shared clustering by extending the likelihood to include the product of the likelihoods of the individual graphs. 

\begin{equation}
  p(\vec{c}, \{\eta^q\}, \{\theta^q\} | \{R^q\} ) \propto \prod_q \Bigg(\prod_{i, j} p(R^q(e_i, e_j) | f(d(e_i, e_j) | \eta^q_{c_ic_j}, \theta^q) \prod_{m, n} p(\eta^q_{mn} | \theta^q)  p(\theta^q) \Bigg) p(\vec{c} | \alpha) p(\alpha) 
\end{equation}

\FloatBarrier

\subsection{Microprocessor Netlist annotation}
\label{supp:mos6502netlist}
This is a matrix consisting of the different clusters of nets for 
the registers, with the colors consistent with figure~\ref{mos6502}

\includepdf[pages=-]{mos6502.clusters.pdf}

\FloatBarrier
\subsection{Model Comparison for Mouse Retina}

We compare the clusters found by our model, the traditional infinite
stochastic block model, and a finite stochastic block model with the number
of clusters set to the best-found number from our iSRM model ($k=12$).

We see from fig~\ref{fig:supp:irmspatialcomparemouse} that the spatial
variance of our clusters closely matches the ground truth, while the
other two models find spatially-localized
clusters. Figure~\ref{fig:supp:mousecompareari} shows the ARI of our
clustering, the clustering found by the iSBM,
as well as the result of parametrically varying the traditional stochastic
block model for various values of $k$. 

The actual clusterings for iSRM, the iSBM, and $SBM_{k=12}$ are shown
in figures \ref{fig:supp:retinabbclusters},
\ref{fig:supp:retinaldclusters}, and
\ref{fig:supp:retinabbfixedclusters} respectively. Each block is a
cluster, with the true cell type indicated along the x-axis, and the
cell ID written to the left or right.


\begin{figure}[h!]
  \centering 
    \centerline{\includegraphics[width=5in]{mouseretina/spatial_var.pdf}}
  \caption{}
\label{fig:supp:irmspatialcomparemouse}
\end{figure}

\begin{figure}[h!]
  \centering 
    \centerline{\includegraphics[width=5in]{mouseretina/ari_vs_cluster.pdf}}
  \caption{}
\label{fig:supp:mousecompareari}
\end{figure}

\begin{figure}[h!]
  \centering 
    \centerline{\includegraphics[width=6.5in]{mouseretina/retina.1.2.bb.0.0.data-fixed_20_100-anneal_slow_400.truth_latent.pdf}}
  \caption{}
\label{fig:supp:retinabbclusters}
\end{figure}

\begin{figure}[h!]

  \centering 
    \centerline{\includegraphics[width=6.5in]{mouseretina/retina.1.2.bb.0.0.data-fixed_20_12-anneal_slow_fixed_400.truth_latent.pdf}}
  \caption{}
\label{fig:supp:retinabbfixedclusters}
\end{figure}

\begin{figure}[h!]

  \centering 
    \centerline{\includegraphics[width=6.5in]{mouseretina/retina.1.2.ld.0.0.data-fixed_20_100-anneal_slow_400.truth_latent.pdf}}
  \caption{}
\label{fig:supp:retinaldclusters}
\end{figure}

\newpage

\FloatBarrier
\subsection{Hyperprior grids}
\label{supp:hyperpriors}

For the mouse retina Logistic-Distance Bernoulli model, we gridded $\mu^{hp}$ and $\lambda^{hp}$ into 40 $\log_{10}$-spaced points 1.0 and 80. $p_{min} \in [0.001, 0.005, 0.01]$ and $p_{max} \in [0.80, 0.90, 0.95, 0.99]$.

For the c. elegans data with the Logistic Distance poisson model, we gridded $\mu_{hp}$ and $\lambda$ into 20 $\log_{10}$-spaced points bween 0.2 and 2.0, and the $ratescale^{hp}$ parameter into 20 $\log_{10}$-spaced points between 2.0 and 20.0. We globally set $rate_{min}=0.01$. 

For the microprocessor with the Logistic Distance fixed lambda Bernoulli likelihood, we gridded $mu_{hp}$ into 50 $\log_{10}$-spaced points between 10 and 500 and set $\lambda=\mu_{hp}/10$. $p_{min} \in \{0.001, 0.01, 0.02\}$  and both $p_\alpha$ and $p_\beta \in \{0.1, 1.0, 2.0\}$. 


\section{Measuring clustering similarity}

The adjusted rand index (ARI) is a measure of the similarity of two
different clusterings \autocite{Hubert1985} -- two identical clusters
have an ARI of 1.0 while progressively more dissimilar clusters have
lower ARIs, becoming negative as the clustering gets anti-correlated.


\end{document}



